<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <meta charset="UTF-8">
  <link href="http://gmpg.org/xfn/11" rel="profile">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>
    
      Data investigation &middot; Smooth Voxel Modeling
    
  </title>

  <!-- CSS -->
  <!-- <link rel="stylesheet" href="/styles.css"> -->
  <link rel="stylesheet" href="http://jdily.github.io/styles.css">
  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/atom+xml" title="Smooth Voxel Modeling" href="/atom.xml">
</head>

  
      <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$']],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    }
  });
</script>
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
  
  <body>

    <div class="container content">
      <div class="masthead">
        <h3 class="masthead-title">
          <a href="/problem" title="Home">Smooth Voxel Modeling</a>

          
              &nbsp;&nbsp;&nbsp;<small><a href="/problem">Problem</a></small>
          
              &nbsp;&nbsp;&nbsp;<small><a href="/progress">Progress</a></small>
          
              &nbsp;&nbsp;&nbsp;<small><a href="/meeting">Meeting Note</a></small>
          
              &nbsp;&nbsp;&nbsp;<small><a href="/draft">Draft</a></small>
          

        </h3>
      </div>

      <article class="post">
  <h1 class="post-title">Data investigation</h1>
  <time datetime="2015-11-07T00:00:00-08:00" class="post-date">07 Nov 2015</time>
  <p>I did a comparison for the laplacian inference using trained laplacian regression function, using linear regression, nearest neighbour and network.
You can find some comparison results in the following webpage (including numerical and shape comparison, ).
<a href="http://www.cs.ubc.ca/~ichaos/research/nbview/exp4-val-linear-sound.html">http://www.cs.ubc.ca/~ichaos/research/nbview/exp4-val-linear-sound.html</a>
<a href="http://www.cs.ubc.ca/~ichaos/research/nbview/exp4-val-linear-guitar.html">http://www.cs.ubc.ca/~ichaos/research/nbview/exp4-val-linear-guitar.html</a>
<a href="http://www.cs.ubc.ca/~ichaos/research/nbview/exp4-val-linear-mickey.html">http://www.cs.ubc.ca/~ichaos/research/nbview/exp4-val-linear-mickey.html</a>
<a href="http://www.cs.ubc.ca/~ichaos/research/nbview/exp4-val-linear-flag.html">http://www.cs.ubc.ca/~ichaos/research/nbview/exp4-val-linear-flag.html</a></p>

<p>There are some confusion about the shape shown in the webpage, they are results of nearest neighbours, if needed I can also show all the results using different methods.</p>

<h4 id="thoughts">Thoughts</h4>

<p>But the conclusion for the comparison is : <strong>there are no huge differences between all these methods</strong>.
The good thing is I think our problem formulation, in the beginning, already help us to reach a good point (the zero predict in the previous results show that even without inferred laplacian value, we can already get a fairly good overall shape.)
What we want to achieve is to infer good local structures (geometry feature) that captured in the data.</p>

<p>So I decided to do some investigation of our input data, especially the training data, with some visualization.</p>

<h3 id="data-distribution---output-laplacian-value">Data distribution - output laplacian value</h3>

<p>First thing that I tried to visualize is the distribution of the ground truth laplacian values in the training set.
I mentioned this before, but never try to visualize them, so here we go.
<img src="http://jdily.github.io/images/value_distribution.png" alt="ground truth laplacian distribution" />
Figure on the left hand side (green bars) is the distribution of <strong>x-component</strong>, and right hand side is the distribution of <strong>y-component</strong>.
According to the figures, I found out that most of our output values are very close to 0.
And there are very few laplacians are with bigger scale, but I think those bigger laplacians are the thing that we care about (because they represent the local feature.).</p>

<h3 id="prediction-value-distribution---test-case-sound">Prediction value distribution - Test case (sound)</h3>
<p>Next thing I did is I plot the distribution of both x- and y-components of output laplacian values in 2D (see the following fig).
(x-axis is for x-components, and y-axis is for y-components).
<img src="http://jdily.github.io/images/output_val_plot.png" alt="output value distribution" />
The blue dots are the ground truth points, the green dots are the predicted results using linear regression, and the red dots are predicted results using network.
As you can see, most of the predictions from both methods gather up close to the center (which is (0, 0) ), and fail to capture the bigger laplacians on the outside.</p>

<h4 id="supplemental-figure--training-value-distribution">Supplemental figure : Training value distribution</h4>
<p><img src="http://jdily.github.io/images/output_val_plot_train.png" alt="output value distribution - training" />
This new figure illustrate the training set fitting.</p>

<h3 id="feature-attributes-and-predict-value">Feature attributes and Predict value</h3>
<p>I havenâ€™t have a very concrete idea of how to show this, but I also plotted something out, so I think it still ok to show it.
In the following two figs, I plotted some dimensions of the input features with a output dimension (e.g. dim 0 in the feature pair with dim 0 in result laplacian.)</p>

<p>This figure shows the relationship between different dimension in features and x-component in result laplacian.
<img src="http://jdily.github.io/images/data_predict_plot_x.png" alt="pair-x" /></p>

<p>This figure shows the relationship between different dimension in features and y-component in result laplacian.
<img src="http://jdily.github.io/images/data_predict_plot_y.png" alt="pair-y" /></p>

<p>So far, these figures just reveal the same thing with the previous figure, that all the predict values gather at centre area.</p>


</article>


<aside class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/draft/2015/12/15/Shape-Repo/">
            Shape List
            <small><time datetime="2015-12-15T00:00:00-08:00">15 Dec 2015</time></small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/meeting/2015/12/09/Dec-09-Meeting-Note/">
            Dec 09, 2015 Meeting note
            <small><time datetime="2015-12-09T00:00:00-08:00">09 Dec 2015</time></small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/meeting/2015/12/02/Dec-02-Meeting-Note/">
            Dec 02, 2015 Meeting note
            <small><time datetime="2015-12-02T00:00:00-08:00">02 Dec 2015</time></small>
          </a>
        </h3>
      </li>
    
  </ul>
</aside>



      <div class="footer">
        <p>
          &copy; 2015. All rights reserved.
        </p>
      </div>
    </div>

  </body>
</html>